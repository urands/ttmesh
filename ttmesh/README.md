# TTMesh

Decentralized task scheduler prototype.

Анализ децентрализованных планировщиков задач и архитектурных решений

Концепция децентрализованного планировщика задач
Цели и основные особенности проекта

Данный проект представляет собой распределённый планировщик задач (framework) с упором на децентрализованность и высокую производительность. Система должна масштабироваться от одного потока в одном процессе до кластера из тысяч узлов, оставаясь при этом единой по интерфейсу. Ключевые цели и возможности системы:

Полная децентрализованность: отсутствует единая точка отказа – узлы сети сами договариваются (через алгоритмы консенсуса) о том, кто будет выполнять ту или иную задачу, и совместно верифицируют результаты выполнения.

Разбиение задач на подзадачи: крупные задания могут автоматически дробиться на подзадачи, распределяемые между исполнителями (аналогично подходам BOINC для добровольных вычислений).

Максимальная скорость выполнения: архитектура оптимизирована для минимизации задержек – используются асинхронные вызовы, корутины C++20 и другие техники, чтобы эффективно загружать все доступные ресурсы (CPU, сеть, и пр.).

Высокая отказоустойчивость: система устойчива к сбоям узлов – при наличии достаточного числа инстансов задачи переназначаются другим исполнителям; результаты задач могут проверяться на корректность путём репликации или голосования.

Гибкость масштаба: решение подходит как для локального запуска (один процесс с многопоточностью/корутинами и общей памятью), так и для распределения работы на 10 000+ серверов (каждый может запускать по несколько процессов-узлов).

Шардирование и приоритеты: поддерживается интеллектуальный выбор исполнителя – например, по географической близости к источнику данных или получателю результата, учёт загрузки узлов, приоритетности задач и др.

Неограниченные объёмы данных: система способна обрабатывать задачи с большим объёмом данных, передавая результаты частями (streaming) по мере готовности. Например, при долгой обработке (кодирование видео, генерация отчетов и т.д.) результаты могут отдаваться клиенту чанками без ожидания окончания всей задачи.

Поддержка различных протоколов и транспортов: единый интерфейс задач абстрагирован от транспорта – коммуникация между узлами может осуществляться по различным протоколам (gRPC, JSON-RPC, двоичный собственный протокол и др.) и через разные брокеры/каналы (общая память, TCP/UDP сеть, очереди сообщений вроде Redis, RabbitMQ и пр.).

Простота использования: для разработчика предоставляется максимально простой API, инкапсулирующий всю сложную логику планировщика. Интерфейсы задач и провайдеров спроектированы единообразно для всех масштабов – от локального режима до распределённого кластера.

Цель – создать встраиваемый фреймворк (на C++20), который впоследствии может стать самостоятельным продуктом, объединяющий лучшие идеи существующих систем планирования задач и распределённых брокеров.

Обзор существующих решений и аналогов

При проектировании необходимо учесть опыт и идеи из уже существующих систем распределённой обработки задач. Вот обзор некоторых релевантных решений и их свойств:

Celery (Python) – широко используемая распределённая очередь задач. Предлагает простой интерфейс для отложенного выполнения задач и масштабирования на несколько рабочих узлов. Celery использует брокеры сообщений (RabbitMQ, Redis и др.) для обмена сообщениями между клиентом и воркерами: клиент помещает задачу в очередь, брокер доставляет сообщение воркеру, который выполняет задачу
github.com
. Поддерживается множество видов транспорта – кроме RabbitMQ и Redis есть эксперим. драйверы (SQLite и др.), а работать Celery может как на одном сервере, так и на множестве машин или даже между датацентрами
github.com
. Ограничение Celery – централизованный брокер: система зависит от доступности очереди (что обычно решается кластеризацией брокера), и нет встроенного консенсуса между воркерами о распределении задач или проверке результатов.

Apache Spark / Hadoop MapReduce – фреймворки для параллельной обработки больших данных. Они разбивают задачу (job) на множество подзадач (tasks) по обработке частей данных и распределяют их по узлам кластера. В Spark реализовано планирование задач с учётом данных (data locality) и отказоустойчивость достигается через повторный запуск незавершившихся задач на других узлах. Однако архитектура этих систем централизована: имеется ведущий узел (driver/master), отвечающий за расписание задач по исполнителям (воркерам). Такие системы хорошо масштабируются для batch-вычислений, но не обеспечивают децентрализованного консенсуса между всеми узлами – есть единая координирующая сущность. Кроме того, Spark/MapReduce рассчитаны на относительно крупные задачи (в секундах и выше) и не предназначены для сверхнизких задержек или стриминга результатов в режиме реального времени.

Ray и Dask (Python) – современные фреймворки для распределённых вычислений, нацеленные на интерактивные и быстрые задачи. Ray предоставляет единый API для вызывных задач (functions) и акторов (состояние + методы) в кластерной среде. Внутренне Ray использует распределённый планировщик и отказоустойчивое хранилище метаданных, чтобы добиться высокой пропускной способности
usenix.org
. В экспериментах Ray достигает масштабирования до ~1.8 миллиона задач в секунду благодаря распределённому контролю и оптимизации передачи данных
usenix.org
. Архитектура Ray всё же включает центральные компоненты (головной узел, хранилище), но планирование частично делегируется локальным диспетчерам на узлах, что уменьшает узкое место. Dask тоже позволяет распределять вычисления (особенно для DataFrame/научных задач), но из коробки полагается на центральный Scheduler-процесс для координации работы, хотя и поддерживает разнообразные планы выполнения.

C++ HPC-фреймворки (HPX, Charm++ и др.) – в экосистеме C++ существуют библиотеки для задачно-ориентированного параллелизма, способные работать в распределённой среде. HPX – это асинхронный runtime, реализующий стандартизированный API для параллельных и распределённых приложений на С++
hpx-docs.stellar-group.org
. HPX поддерживает futures, async/await, планирование задач по узлам (“parcelport” через сеть) и даже имеет модуль Resiliency для обработки сбоев. Например, HPX может выполнять репликацию задачи на нескольких узлах/потоках и затем собрать результаты, определяя консенсус (правильный результат) среди них
hpx-docs.stellar-group.org
hpx-docs.stellar-group.org
. Этот подход повышает надёжность вычислений: если одна копия задачи дала неверный или отличающийся результат (например, из-за аппаратной ошибки), остальные копии переголосуют “правильное” значение. Подобная функциональность (replicate/vote) позволяет повысить доверие к результатам в случае недостоверных сред, хотя увеличивает накладные расходы
hpx-docs.stellar-group.org
hpx-docs.stellar-group.org
. Charm++ – другой HPC-фреймворк на основе модели акторов (chares), поддерживающий динамическое балансирование нагрузки между узлами. Charm++ централизованно не выбирает исполнителей для каждой задачи – объекты могут мигрировать сами при дисбалансе. Однако ни HPX, ни Charm++ не встроены в блокчейн-среду и предполагают контролируемый кластер (доверие к узлам), поэтому консенсус в доверенной среде проще (достаточно детектировать ошибки и перепланировать).

Оркестраторы кластеров (Nomad, Kubernetes) – системы, изначально предназначенные для развёртывания сервисов и контейнеров, но обладающие релевантными идеями. Например, HashiCorp Nomad – распределённый оркестратор, который работает без отдельной БД, используя Raft-консенсус между несколькими серверными узлами для хранения состояния кластера. В Nomad несколько серверов совместно реплицируют состояние (jobs, аллокации) и выбирают лидера, что обеспечивает непрерывность работы при сбоях (failover)
blog.nashtechglobal.com
. Клиентские узлы (workers) подключаются к серверам и получают задания. Nomad показывает, что можно достичь отказоустойчивого планирования без единственной точки отказа: все серверы держат консистентную копию данных о задачах благодаря алгоритму Raft
blog.nashtechglobal.com
. Наша система планировщика заимствует эту идею консенсуса для координации узлов. Kubernetes также обеспечивает высокую доступность через etcd (внутреннее хранилище на основе Raft), но по архитектуре имеет единый планировщик задач (Scheduler) в master-компоненте. Для высокой скорости реакций Kubernetes менее подходящ – он рассчитан на минутные развертывания, а не на миллисекундные таски.

Добровольческие вычисления (BOINC) – пример распределённой системы, где узлы (добровольцы) не доверены, и для доверия результатам применяется избыточность. BOINC (платформа для проектов типа SETI@home) разбивает большую задачу на множество небольших независимых job’ов, рассылает их разным участникам и затем валидирует результаты. Верификация часто осуществляется путём репликации: один и тот же job отправляется нескольким узлам, и если ответы от двух (или более) хостов совпадают, результат считается корректным
boinc.berkeley.edu
. В противном случае посылаются дополнительные копии до достижения консенсуса большинства или до исчерпания попыток
boinc.berkeley.edu
. Такой подход решает проблему недостоверных узлов, ценой увеличения общего объёма вычислений. BOINC также умеет подбирать исполнителей по их производительности, отслеживать ошибки и пр. – эти идеи полезны для нашего проекта (в части репликации важных задач и разбиения больших вычислений на подзадачи).

Блокчейн-платформы для вычислений (Golem, iExec, SONM) – полностью децентрализованные сети, где участники предоставляют вычислительные ресурсы, а задачи распределяются через рынок на основе смарт-контрактов. Golem позиционируется как «AirBnB для компьютеров», создавая P2P-сеть без центрального сервера
decrypt.co
. Архитектура Golem предусматривает, что разработчики готовят специальный шаблон задачи, описывающий код вычисления, как его разделить на части и как собрать/проверить результаты
researchgate.net
. Исполнители (providers) запускают задания изолированно (например, в Docker-контейнерах или VM), используют распределённое хранилище (на базе IPFS) для обмена данными, и возвращают результаты заказчику
researchgate.net
researchgate.net
. Для проверки правильности Golem тоже применяет репликацию и сверку результатов (или другие методики, заданные в шаблоне задачи). iExec реализует похожую идею децентрализованного облака: у них есть узлы-Schedulers (распределённая сеть планировщиков), которые формируют пул воркеров и назначают им задачи, а также считают рейтинг надёжности этих воркеров
gemini.com
gemini.com
. iExec ввёл механизм Proof-of-Contribution (PoCo) – специальный консенсус, который записывает на блокчейн факт выполнения задания и валидирует вклады исполнителей (например, через репутацию, проверочные задачи или доверенное выполнение)
gemini.com
gemini.com
. Эти проекты показывают, как можно добиться доверия в недоверенной среде: комбинацией криптографических доказательств, репутационных систем и экономических стимулов (токены). Однако они добавляют накладные расходы (смарт-контракты, платежи) и сложность интеграции. Наш планировщик может взять некоторые идеи блокчейна – например, неизменяемый лог задач и результатов, децентрализацию принятия решений – но без обязательного использования криптовалют.

Вывод: ни одно из существующих решений в полной мере не покрывает всех наших требований, но многие предоставляют ценные идеи. Мы будем опираться на них при разработке архитектуры: возьмём унифицированный интерфейс из фреймворков вроде Ray, плагинную модель брокеров как в Celery, консенсус и отказоустойчивость как в Nomad, дробление задач и валидацию результатов как в BOINC/Golem, а реализуем всё это на современном C++ для эффективности, опираясь на опыт HPC-библиотек (HPX и др.).

Архитектура системы и компоненты

Архитектура предлагаемого планировщика модульная, чтобы обеспечить адаптивность под различные масштабы. Основные составляющие системы:

Узел (Node): базовая единица распределённой системы. Один узел представляет собой запущенный процесс нашего сервиса. Узел может выполнять роли планировщика задач и исполнителя (worker) одновременно (в децентрализованной среде каждый узел равноправен, хотя могут быть выделены специальные роли при необходимости). В режиме одного процесса единственный узел содержит и планировщик, и исполнители (потоки/корутины). В кластере – множество узлов соединены по P2P-сети, обмениваются сообщениями о задачах и состоянии.

Локальный диспетчер задач: компонент узла, отвечающий за выполнение задач на самом узле. Он управляет пулами потоков и очередями, планирует задачи для выполнения локально (с учётом приоритетов, зависимостей и пр.). В однопоточном режиме это может быть просто цикл событий, выполняющий корутины последовательно. В многопоточном – планировщик с Work Stealing между потоками (как в многих фреймворках) для оптимальной загрузки CPU. Этот диспетчер также может решать, какие задачи готовы к отправке другим узлам, и принимать задачи, делегированные внешними узлами.

Глобальный механизм планирования (распределённый): совокупность протоколов, позволяющих узлам договориться, кто и где будет выполнять каждую задачу. Здесь ключевая особенность – отсутствие единого центрального координатора; вместо этого узлы достигают консенсуса. Реализовано это может быть с помощью:

Рафт/Paxos-кластера из узлов: Например, несколько узлов могут выполнять роль координаторов, храня совместное состояние системы (список задач, очередь на исполнение, занятость узлов). Они реплицируют эти данные между собой и принимают решения большинством голосов. При появлении новой задачи запись о ней через консенсус заносится в общий лог, после чего назначается исполнитель. Такой подход аналогичен Nomad/etcd (Raft) – даёт сильную консистентность и устойчивость к отказам координаторов
blog.nashtechglobal.com
. Недостаток – небольшое снижение производительности из-за раундов консенсуса, но для надёжности это оправдано.

Гossip и локальные решения: Дополнительно может использоваться протокол gossip (слухи) для обмена нагрузкой и состоянием узлов. Например, каждый узел периодически сообщает соседям о своей загруженности, а новые задачи рассылаются по принципу эпидемического распространения. Исполнителя можно выбирать детерминированно (например, хеш-задачи указывает ответственного узла, как в распределённых хэш-таблицах) или устраивать краткий раунд голосования между кандидатами. Gossip хорошо масштабируется и не требует центрального узла, но обеспечивает лишь конфигурационную согласованность (eventual consistency) – для критичных задач мы будем опираться на более строгий Raft-подобный консенсус, а gossip-подход использовать для оптимизации (быстрое уведомление, кто свободен и т.п.).

Выбор исполнителя и балансировка: Алгоритм распределения задач учитывает метаданные задачи (например, требуемые ресурсы, гео-метки) и состояние узлов. Может использоваться подход как в Ray – сначала попытка назначить локально или на “топ-K” подходящих узлов
docs.ray.io
, чтобы сохранить кэш данных или снизить трафик, а в случае отказа – перекинуть на другой. Если задача имеет флаг гео-привязки, планировщик постарается выбрать узел в нужном регионе. Приоритеты задач также учитываются: высокоприоритетные обходят очередь или исполняются параллельно при наличии ресурсов.

Компонент консенсуса/репликации результатов: Чтобы доверять результатам в среде, где узлы могут выйти из строя или дать неверный результат, предусмотрены механизмы верификации:

Репликация задачи: по аналогии с HPX и BOINC, для критических задач (или при подозрении на сбой) узел-планировщик может запустить несколько копий задачи на разных узлах параллельно. Результаты затем сравниваются: если результаты идентичны – принимается, если различаются – может выполняться дополнительный запуск или применяться функция голосования. Пользователь может задать функцию консенсуса (например, большинство или определённая проверка допусков), которая решит какой результат считать корректным
hpx-docs.stellar-group.org
. Этот подход увеличивает надёжность вычислений
hpx-docs.stellar-group.org
. Разумеется, репликация делается опционально для задач, где цена ошибки высока (например, расчёт научных данных) или узлы недоверенные.

Перезапуск (replay) на ошибке: при падении узла во время выполнения задачи система обнаружит отсутствие подтверждения и автоматически перепланирует задачу на другой ресурс. Благодаря хранению прогресса (см. ниже про данные) возможен checkpoint/restart, если задача умеет сохранять состояние.

Репутация узлов: для долгоживущей сети можно ввести простую систему метрик/рейтинга узлов (вдохновлено iExec
gemini.com
) – учитывать процент успешно выполненных и верифицированных задач. Планировщик будет предпочитать узлы с высокой надёжностью для важных задач, а узлы с частыми сбоями – либо получать меньше задач, либо им назначаются контрольные вычисления для проверки. Это повышает доверие без полного блокчейна, но в нашей системе это факультативно и скорее статистика.

Хранилище метаданных задач: информация о заданиях (их статус, назначенный исполнитель, приоритет, результаты) должна быть доступна всем узлам. Мы планируем два уровня хранения:

Локальное представление: каждый узел хранит у себя кэш информации о задачах, которые он выполняет или выдал, плюс о состоянии ближайших соседей/ответственных (через gossip).

Глобальное согласованное хранилище: на основе Raft-консенсуса – например, распределённый ключ-значение storage, который держат несколько узлов-координаторов. В нём фиксируется неизменяемый лог поступления задач и подтверждения их выполнения (аналогично блокчейн-реестру, но приватному). При расхождении локальных данных доверяем глобальному консенсус-логу. Это хранилище также позволяет новым узлам подключаться и получать актуальное состояние системы.

Менеджер ресурсов и мониторинг: компонент, следящий за загрузкой узла (CPU, память, сеть) и доступностью соседних узлов. На основе этой информации планировщик принимает решения о распределении (не отправлять задачу на перегруженный узел, дождаться свободного или раздробить задачу между несколькими узлами). Также мониторинг позволяет быстро реагировать на отпадание узла – задачи, числившиеся за ним, будут помечены невыполненными и отправлены на повторное планирование.

Диаграмма ниже иллюстрирует упрощённо устройство системы: каждый узел содержит локальный планировщик/исполнитель, все узлы связаны через абстракцию провайдера (транспорта), над которыми работает уровень консенсуса и глобального планирования:

【Диаграмма архитектуры распределённого планировщика†embed_image】Примерная схема компонентов децентрализованного планировщика задач (узлы Node A, B, C обмениваются задачами через провайдер; консенсус-слой согласует состояние задач).
blog.nashtechglobal.com
github.com

(На диаграмме: каждый Node имеет локальные очереди задач (Task Queue) и исполнители (Workers). Компонент Scheduler+Consensus координирует с другими узлами через Network Provider. Данные задач могут сохраняться в Distributed Storage (лог задач). Пользователь через API отправляет задачу в любой узел, далее она распределяется по сети.)

Планирование задач, консенсус и отказоустойчивость

Децентрализованное планирование. Когда поступает новая задача (например, клиент вызвал удалённую функцию или сработал триггер), узел-инициатор решает, как её выполнить:

Локальное исполнение: Если ресурс данного узла достаточен и задача не требует удалённых данных, она может быть сразу помещена в локальную очередь на исполнение. Это минимизирует накладные расходы на передачу.

Удалённое исполнение: Если задача помечена для определённого пула ресурсов (например, GPU, который есть только на некоторых узлах, или определённое гео), либо локально нет свободных слотов, узел ищет исполнителя на стороне. Он рассылает запросы кандидатам или обращается к координатору (Raft-группе) с просьбой назначить исполнителя.

При прямой рассылке может использоваться алгоритм вроде Auctions: узел-раздатчик предлагает задачу, а узлы-исполнители откликаются, кто готов взять (с учётом своей загрузки). Либо же инициатор сам выбирает наиболее подходящего по актуальной информации (например, минимальный RTT и достаточная память).

При обращении к координаторам – задача пишется в глобальный лог задач, и один из координаторов (лидер) назначает её конкретному узлу, после чего все узлы получают обновление через консенсус. Этот способ надёжнее (все знают о назначении), но медленнее из-за раунда записи.

Консенсус назначения: Чтобы не было противоречий (двойного исполнения без необходимости), назначение задачи на узел подтверждается либо большинством участников (например, инициатор ждет подтверждений от N других узлов, что “ОК, узел X будет делать”), либо записью в консенсус-хранилище. В простейшем случае – достаточно, чтобы сам исполнитель согласился и зафиксировал это. Однако для важной задачи может требоваться, чтобы, например, 3 координатора записали факт назначения (аналог сделки в блокчейне, но приватной).

Исполнение и контроль: Назначенный узел выполняет задачу. По окончании он отправляет результат инициатору (и/или в общий лог). Другие узлы могут временно отслеживать, что задача в работе (например, таймаут). Если превышено время или узел-исполнитель не откликается, происходит репланирование: задача помечается как не выполненная и выдаётся другому узлу (либо реплицировалась заранее на несколько – тогда просто берётся результат от резервного).

Подзадачи: Если задача разбивается на части (см. ниже), то планирование происходит рекурсивно для каждой части, а сбор результатов – на узле-координаторе этой задачи.

Алгоритмы консенсуса. Как уже отмечалось, для поддержания целостности данных о задачах применим алгоритм Raft или подобный (Paxos). Мы можем запустить несколько узлов в режиме координационного кворума – они будут согласовывать между собой изменения состояния. Например, добавление новой задачи в систему – это транзакция, которая должна быть подтверждена большинством координаторов. Данные о задаче (ID, параметры, назначенный исполнитель) реплицируются на них всех. Если лидер координаторов падает, другой занимает его место и продолжает с последнего согласованного состояния
blog.nashtechglobal.com
. Так достигается отказоустойчивость метаданных. В случае гигантского кластера (1000+ узлов) не обязательно все узлы быть координаторами – достаточно 3-7 координаторов, остальные могут доверять им. Однако архитектура позволит и полностью безлидерный консенсус (например, HoneyBadgerBFT или др.) при необходимости толерантности к византийским отказам, хотя это значительно сложнее.

Отказоустойчивость и восстановление. Система спроектирована так, чтобы продолжать работу при частичных сбоях:

Если отключается/падает исполнитель, не завершив задачу, то по таймауту другие узлы обнаружат, что результат не получен. Задача будет снова поставлена в очередь. При интеграции с консенсус-логом координаторы могут пометить старую попытку как "failed" и разлочить задачу для повторного исполнения. Репликация задач (запуск сразу нескольких копий) заранее смягчает этот сценарий – если один узел упал, другой, возможно, уже дорабатывает.

Если выходит из строя узел-координатор (участвовавший в Raft-кворуме), оставшиеся координаторы автоматически выберут нового лидера и продолжат. Задачи, закреплённые за упавшим узлом, будут переназначены.

Для сохранения промежуточного прогресса долгих задач можно использовать контрольные точки (checkpointing): задача периодически сообщает о прогрессе и сохраняет state либо в распределённое хранилище, либо передаёт инициатору. Тогда при сбое продолжительность уже выполненной части не потеряется.

Изоляция ошибок: Если узел начал выдавать некорректные результаты (в отличие от других реплик), его могут автоматом поместить в карантин (не давать важных задач) и сигнализировать администратору. Таким образом, система не рухнет из-за одного "плохого" узла.

Масштабирование и производительность планировщика. Для обеспечения максимальной скорости мы используем:

Локальное неблокирующее планирование: во главе угла – асинхронность. На узле задачи расписаны через очередь, исполняются либо в виде корутин (co_await) без блокировки потоков, либо в пуле потоков с минимальным переключением контекста. Сеть обрабатывается в неблокирующем режиме (например, на основе epoll/IOCP). Это позволяет параллельно обработать огромное число мелких задач. По аналогии с результатами Ray, наша цель – миллионы задач в минуту на кластер без существенных задержек на планирование.

Устранение глобальных блокировок: отсутствие центрального диспетчера означает, что узлы могут принимать задачи параллельно. Консенсус-операции по Raft происходят асинхронно и, по возможности, пачками (batched commits), чтобы не блокировать каждый таск. Например, можно объединять информацию о нескольких новых задачах в один раунд консенсуса.

Кэширование решений: если ранее задача схожего типа уже распределялась на определённые узлы, можно повторно использовать тот же узел (assuming он свободен), чтобы не тратить время на новый выбор. Также данные, которые недавно запрашивал узел, могут находиться в его памяти, поэтому повторную задачу с теми же данными целесообразно отправить туда же (data locality).

Модель задач: разбиение, зависимости и шардирование

Система поддерживает разные типы задач, чтобы охватить как простые отдельные задачи, так и сложные вычислительные пайплайны:

Простая задача (Job): аналог удалённого вызова функции – принимает входные данные, после выполнения возвращает результат. Такие задачи могут выполняться параллельно и независимо (если не указаны зависимости). Планировщик рассматривает каждый job как наименьшую единицу планирования, которая может быть назначена на один узел.

Составная задача (DAG или суб-задачи): если вычисление можно разделить на части, разработчик может определить иерархию задач. Например, большая задача имеет N одинаковых подзадач (map) и финальную стадию агрегации результатов (reduce). Фреймворк должен уметь исполнить подобный паттерн: подзадачи распределяются по узлам, выполняются параллельно, а их результаты отправляются на узел-сборщик, который выполняет финальную часть. Эта схема похожа на MapReduce/Spark, но в нашем случае можно более гибко определять структуру (не только две стадии). Мы можем представить задачу в виде DAG (ориентированного ацикличного графа): узлы графа – подзадачи, рёбра – передачи данных/зависимости. Планировщик вычисляет топологический порядок выполнения, удовлетворяющий зависимостям. Параллельно могут исполняться узлы без зависимостей друг от друга. Если часть DAG огромна (например, тысячи мелких tasks), есть смысл распределить её по кластеру.

Пример: анализ видео может быть представлен как DAG: узел 1 – декодирование видео на кадры, узлы 2..N – обработка отдельных кадров (параллельно), узел N+1 – сбор результатов. Узел 1 начинает, по мере готовности кадров задачи отправляются обработчикам, результаты стекаются на финальный узел. Всё это координируется нашим планировщиком автоматически.

Стриминговая или длительная задача: некоторые задачи не имеют конечного результата сразу, а выдают поток данных. Например, задача типа “служить видеофрагменты по запросу клиента” – по сути, продолжительная задача, которая генерирует выход (байтовые буферы видео) частями. В нашей архитектуре такие задачи могут быть реализованы двумя способами:

Актор (persisting actor): Задача-актор, которая запускается на узле и остаётся активной, ожидая запросов. Клиент может взаимодействовать с актором через сообщения (RPC). Актор может отправлять обратно поток данных. Например, запущен актор CDNVideoServer на узле ближнем к пользователю; клиент запрашивает у него сегменты видео – актор читает/генерирует и посылает chunk’и. Этот подход аналогичен модели actor в Ray или распределённым сервисам.

Генератор (stream task): Задача обозначается как потоковая – тогда протокол передачи результатов подразумевает, что результат приходит не одним куском, а серией сообщений. На уровне API разработчик может итерировать выдачу (yield в корутине). Планировщик при этом удерживает сессию с клиентом открытой и пересылает данные по мере поступления. Внутренне это похоже на gRPC streaming: устанавливается канал, данные идут частями.

В обоих случаях важно, чтобы транспорт (см. ниже) поддерживал длительные соединения и back-pressure (в случае, если потребитель не успевает получать данные, чтобы не переполнить память).

Параметризованные задачи и шардирование данных: Чтобы эффективно использовать ресурсы, система должна поддерживать шардирование – распределение частиц данных по узлам. Если задача работает с огромным объёмом данных (например, база данных, файл), целесообразно разрезать эти данные на блоки и обрабатывать на разных узлах, возможно расположенных ближе к месту хранения этих данных (data affinity). Планировщик может предоставлять механизм: разработчик указывает, как разделить входные данные на части (например, 1000 строк из БД на 10 узлов по 100) – а система создает соответствующие подзадачи автоматически. Также можно динамически шардинг делать: узлы сами запрашивают себе следующую порцию данных, когда готовы (pull-based distribution), что предотвращает простои.

Приоритеты и классы сервисов: Задачи могут иметь приоритет (число или класс). Планировщик реализует очереди с приоритетом локально и глобально: высокоприоритетные задачи берутся в работу раньше. Также возможно резервирование ресурсов под определённые задачи – например, real-time класс (низкая задержка) vs batch класс (фоновые задачи). Узлы могут иметь теги, какие классы они обслуживают (например, узлы с GPU – для ML задач). Все эти параметры учитываются при принятии решений о планировании.

Резюмируя, модель задач гибкая: позволяет выразить как простые единичные задачи, так и сложные распределённые вычисления. При этом разработчику предоставляются абстракции (DAG, акторы, итераторы), а детали распределения по узлам и синхронизации берет на себя планировщик.

Коммуникация между узлами и провайдеры транспорта

Абстракция провайдера. В системе предусмотрен слой абстракции для межузлового взаимодействия – назовём его Provider (провайдер транспорта). Идея в том, чтобы можно было подставить различную реализацию коммуникации, не меняя логики задач:

В простейшем случае (однопроцессном) провайдер – это просто локальная очередь в памяти, через которую планировщик “отправляет” задачу исполнителю (в реальности они в одном адресном пространстве, поэтому передача – это вызов функции или перемещение объекта).

Для нескольких процессов на одной машине – провайдер может использовать Shared Memory (общую память) или межпроцессные очереди. Например, создать разделяемую область памяти, куда один процесс пишет задачи, другой читает. Либо использовать IPC механизм ОС (pipeline, UNIX-socket) – абстракция скроет детали.

Для коммуникации по сети – провайдер может работать на основе TCP/UDP сокетов, gRPC, HTTP или даже специализироваться на брокерах вроде Redis/RabbitMQ. Например, можно реализовать провайдер, который публикует задачи в Redis-канал, а воркеры подписаны на него – получая задачи, они возвращают результат в другой канал. Другой провайдер может напрямую установливать gRPC-соединение между узлами и слать protobuf-сообщения (минуя внешние брокеры).

Также возможно задействовать ZeroMQ или аналогичные библиотеки, которые дают шаблоны request-reply, pub-sub – это удобно, т.к. ZeroMQ сам управляет сокетами и очередями в фоне.

Дизайн провайдера предусматривает единый интерфейс, например:

class ITaskProvider {
public:
virtual void sendTask(const Task& task, NodeID target) = 0;
virtual void broadcastTask(const Task& task) = 0;
virtual void sendResult(const Result& res, NodeID requester) = 0;
// ... возможно, callbacks onTaskReceived и т.д.
};

Каждая реализация (SharedMemoryProvider, GrpcProvider, RedisProvider и пр.) реализует эти методы по-своему. Планировщик пользуется интерфейсом, не завися от конкретного транспорта. Это облегчает расширение: можно подключить новый брокер или протокол, реализовав этот интерфейс.

Форматы данных и RPC протоколы. Несмотря на разнообразие транспортов, необходимо определить общий протокол обмена сообщениями между узлами:

Сериализация задач и результатов: Задача (Task) включает идентификатор, имя/тип действия, входные данные (может быть бинарный блок или JSON, или даже ссылка на большой объект), возможно, требования (CPU/GPU, приоритет). Результат содержит ID задачи, статус (успех/ошибка) и данные результата. Эти структуры должны сериализоваться. Мы можем использовать Protocol Buffers – как для gRPC, так и вне его (protobuf-хранилище легко передать по любому байтовому каналу). Либо, для простоты, JSON для задач (но это менее эффективно). Для бинарных данных – инкапсулировать их (например, Base64 для JSON, или передавать внеполосно).

gRPC: Если выбран gRPC-провайдер, то определяются сервисы, например SchedulerService с методами SubmitTask, SendResult, Heartbeat и т.п. gRPC хорош тем, что поддерживает streaming из коробки – можно реализовать поток результатов как Server-side stream.

JSON-RPC/HTTP: Можно также позволить взаимодействие по HTTP + JSON (например, для интеграции с веб-клиентами). JSON-RPC 2.0 – простой протокол, его можно реализовать поверх HTTP/WebSocket. Этот вариант полезен для простоты и отладки, но в продакшне менее эффективен, поэтому его роль – возможно, интерфейс для внешних клиентов или админки.

Собственный двоичный протокол: Для максимальной производительности, можно разработать легковесный бинарный протокол поверх TCP. Например, фиксированный заголовок (ID, флаги) и затем длина + данные. Это уберёт накладные расходы gRPC. Однако поддерживать такой протокол сложнее, поэтому на первом этапе разумно опереться на существующие (gRPC).

Маршрутизация сообщений. В децентрализованной системе сообщение (например, "задача X назначена узлу Y") должно быть доставлено нужному узлу. Если используем брокер (RabbitMQ/Redis), то маршрутизация реализуется через топики/очереди (воркер Y слушает свою очередь). В P2P-сети – нужен механизм определения адреса узла по NodeID. Можно встроить простой Service Discovery: при запуске узлы регистрируются (в консенсус-хранилище или через мультикаст) и обмениваются адресами и публичными ключами. Затем любой узел может установить прямое соединение с другим (например, по IP:порт). Возможно также поддерживать топологию: либо каждый узел знает про всех (в средних кластерах ок), либо строится оверлей (ring, tree) для масштабирования – но это усложняет доставку (нужно пробрасывать сообщения через цепочки). Пока предполагаем полный mesh по соединениям или звезду через брокер.

Streaming данных и большие объёмы. Особое внимание – передача больших данных (файлы, видео и др.):

Передача чанками: Вместо ожидания полного результата, узел-исполнитель может отправлять данные частями. Наш протокол должен поддерживать множественные сообщения результата. Например, задача "отдай 10-гигабайтный файл" – исполнитель читает файл кусками по 1 МБ и в каждом сообщении отправляет последовательность байтов. При этом он помечает, есть ли ещё данные или это последний кусок. На стороне получателя (клиент или следующий узел) эти куски собираются/передаются дальше. Благодаря этому, задержка первого байта результата минимальна, и память не переполняется от хранения всего сразу.

Direct передачи (минуя координатор): Если данные очень крупные, может быть неэффективно гнать их через лишние узлы. Можно применить технику direct transfer: результат отправляется не обратно тому, кто задачу назначил, а сразу конечному потребителю. Например, клиент запросил видео через узел А, планировщик выбрал узел B (ближайший к файлу) для исполнения. Тогда B после обработки шлёт поток видеоданных напрямую клиенту (по указанному адресу/соединению), минуя A. Для этого A может инициировать ustanovku прямого соединения между B и клиентом (выдать B адрес клиента). Это снижает нагрузку на сеть кластера. Подобное делают CDN и WebRTC (обмен адресами для P2P).

Протоколы для больших данных: Можно интегрировать HTTP(S) для отдачи файлов или даже IPFS, если требуется распределённое хранение. Но в рамках планировщика, скорее всего, будет реализовано как часть логики задачи: задача сама решает, как отдавать. Наша задача – не мешать этому и предоставить канал.

Безопасность передачи. В распределённой системе критично обеспечить безопасность: аутентификацию узлов, шифрование трафика, опционально – шифрование самих задач (если отправляются через публичные брокеры). Решения:

Использование TLS (для gRPC/HTTPS – встроено) с проверкой сертификатов узлов.

Подписание сообщений – например, каждое сообщение содержит цифровую подпись отправителя (ключ узла), проверяемую получателем. В сочетании с консенсусом, это предотвратит подделку команд.

Изоляция данных: при работе через брокеры типа Redis/RabbitMQ – настроить ACL, чтобы посторонний не мог подписаться на приватные очереди.

Суммарно, слой коммуникации – гибкий и многоуровневый. Он позволит запускать систему в разных средах: от встроенного в приложение (где провайдер – просто функция вызова), до облачного кластера (где провайдер – полнофункциональный сетевой протокол с шифрованием и маршрутизацией).

Интерфейсы: API задач и интеграция

Одним из приоритетов проекта является простота интерфейсов для пользователей (разработчиков, которые будут писать задачи с использованием нашего фреймворка) и для интеграторов (кто будет расширять транспортные провайдеры или другие компоненты).

Интерфейс задачи (Task API): Планируется предоставить высокоуровневое API, скрывающее распределённость. Например:

Пользователь может объявить функцию или метод как задачу, аналогично декоратору @task в Celery. В С++ это может быть макрос или шаблон, регистрирующий функцию. Пример:

DistributedTask<int(int,int)> add("AddTask", [](int a, int b){
return a + b;
});
// ...
future<int> result = add(5, 7); // вызов задачи, возвращает future

Здесь DistributedTask – шаблон, который под капотом знает, как сериализовать аргументы, отправить задачу и получить результат (future). Для пользователя этот вызов выглядит почти как обычная функция.

Возможна интеграция с std::future и co_await: т.е. co_await add(5,7) при поддержке coroutines. Тогда задача может вызываться из корутины асинхронно.

Для более сложных сценариев (DAG) – можно предоставить DSL или API для зависимостей:

auto t1 = spawnTask(Job1, data1);
auto t2 = spawnTask(Job2, data2);
auto t3 = spawnTask(Job3, await(t1), await(t2)); // t3 зависит от результатов t1 и t2

Здесь await(t1) указывает, что результат t1 нужен как вход для t3, т.е. система построит DAG и выполнит t1, t2 в параллель, а потом t3.

Для акторов – можно предложить класс DistributedActor:

struct MyService : DistributedActor {
void onStart() { ... }
RPC(int, doWork, string data) { /_ ... _/ }
};
MyService service = spawnActor<MyService>(nodeId);
auto reply = service.doWork("hello"); // RPC вызов

Фреймворк за кулисами отправит RPC на нужный узел, дождётся ответа. Внутри актор может сохранять состояние между вызовами.

Стриминг: в С++ можно реализовать генератор через корутины (кооперативные):

DistributedTask<Generator<Chunk>(VideoRequest)> streamVideo("VideoTask", [](VideoRequest req){
for (auto chunk : VideoEncoder(req)) {
co_yield chunk;
}
});
for await (Chunk ch : streamVideo(request)) {
process(ch);
}

Конструкция for await (псевдокод) обрабатывает поток асинхронно. Конечно, это требует поддержки coroutines (C++20) и соответствующих типов (Generator).

Интерфейс провайдера: Для расширяемости разработчики смогут добавлять новые провайдеры транспорта, реализуя ITaskProvider, как обсуждалось. Мы позаботимся, чтобы взаимодействие планировщика с провайдером было ясным: планировщик вызывает sendTask – провайдер доставляет, по завершении исполнитель вызывает sendResult – провайдер возвращает. Также должны быть события: провайдер по приходу новой задачи дергает callback планировщика, чтобы тот поставил задачу в локальную очередь. Документация опишет требования: гарантии доставки, ограничения по размеру сообщений и т.д., чтобы разработчики провайдеров учитывали их.

Интеграция и развёртывание: Фреймворк должен легко встраиваться:

Библиотека: Предоставляем как библиотеку (статическую/динамическую), которую можно подключить к С++ проекту. Она будет содержать все вышеописанные механизмы. Разработчик может запустить локальный планировщик в своём процессе и зарегистрировать задачи.

Самостоятельный сервис: Также будет возможность запустить наш планировщик как отдельный процесс-сервис (демон) на сервере. Тогда задачи можно регистрировать динамически (например, через загрузку плагинов или отправку описаний задач). Клиенты смогут подключаться по RPC и отправлять задачи в этот сервис. Такой режим похож на serverless-платформы.

Конфигурация: Интерфейсы позволят задавать конфиг: какой провайдер использовать (например, в тестах – InMemory, в проде – Grpc), сколько потоков выделять, список координаторных узлов и др. Настройка может быть через файл или API.

Языковая интероперабельность: Основная реализация – на C++ для максимальной производительности. Но хорошо бы подумать о возможности вызывать задачи из других языков. Например, Python bindings (чтобы Python-скрипт мог отправить задачу в кластер) – возможно через REST/gRPC gateway. Это расширит применение, однако на первом этапе не обязательно, главное – сам движок.

В целом, интерфейсы спроектированы так, чтобы пользователь решал свои прикладные задачи, не задумываясь о распределённости. Например, если он запустил 100 задач, фреймворк сам решит, на каких узлах они выполнятся, как балансировать – прозрачно. При этом для особо требовательных пользователей можно оставить ручное управление: например, возможность указать, на каком узле выполнить (override планировщика), или запустить задачу с репликацией (для критических вычислений). Но по умолчанию разумные решения принимает система.

Выбор технологий и подходов реализации

При разработке данного фреймворка целесообразно использовать современные технологии, соответствующие требуемой функциональности:

Язык и стандарты: выбор пал на C++20, так как он обеспечивает низкоуровневый контроль для производительности и в то же время включает удобные средства для асинхронности – корутины (co_await). Корутины C++20 позволят реализовать неблокирующий ввод-вывод и генераторы, необходимые для стриминга результатов, в относительно понятном синтаксисе. Также C++ имеет богатую экосистему библиотек для сетевого программирования (Boost.Asio и др.) и консенсуса (реализации Raft), что ускорит разработку. Возможно использование элементов будущего C++23/26 (например, стандартные executors, сетевой модуль) по мере необходимости.

Сетевая библиотека: Базовым строительным блоком для коммуникаций может быть Boost.Asio – мощный подход к реализации как синхронных, так и асинхронных сетевых операций, таймеров, работы с потоками. Asio хорошо интегрируется с корутинами C++20, позволяя писать код почти как синхронный (co_spawn, co_await на async операции). Поверх Asio можно построить как собственные протоколы (TCP messaging), так и внедрить сторонние (например, адаптер для nanomsg/ZeroMQ).

Для gRPC интеграции – Google предоставляет официальную библиотеку gRPC C++ (на базе HTTP/2). Мы можем сгенерировать stubs для RPC и использовать их. gRPC в C++ также может работать асинхронно (через CompletionQueue, которую тоже можно в корутину обернуть).

При необходимости HTTP/JSON – библиотека Boost.Beast (HTTP/WebSocket) или более высокоуровневые REST SDK. Но, вероятно, JSON-RPC можно реализовать поверх Beast.

Алгоритмы консенсуса: Реализация Raft с нуля – нетривиальная задача, но есть готовые реализации и библиотеки:

Можно использовать библиотеку ETCD (распространяется как отдельный сервис etcd, но есть embed варианты или REST API) для хранения состояния. Однако в C++ integration с etcd – это либо через REST, либо через existing client libs (которые на С++ редки).

Есть проекты, как Consul (HashiCorp) – но он тоже отдельным процессом.

Можно взять более лёгкую реализацию, например, RAFT library in C++: существует OpenRaft, или использовать Redis Raft module (Redis как консенсус-сервер).

Если нужен византийский консенсус (BFT) – есть например libHotStuff, но это усложнение не нужное на первых этапах.

Наиболее практично: начать с простого master-backup режима (активный координатор и пассивный), а затем улучшить до полного Raft.

Хранилище данных/лог: Для журналирования задач и хранения результатов можно применить легковесную БД. Например, SQLite в режиме WAL для локального лога + периодическая синхронизация между узлами. Но лучше избегать внешних зависимостей – можно реализовать собственный лог-файл + снапшоты состояния. Raft, кстати, предполагает лог, поэтому это совместится: координаторы пишут в файл log of tasks, который можно потом проанализировать для аудита.

Для крупной системы – можно интегрировать Cassandra или ScyllaDB (как в статье Medium, где для 50 млн задач/сутки выбрали Cassandra для расписаний). NoSQL база хорошо масштабируется, но привносит свою сложность. Возможно, наш проект на первых порах обойдётся без этого.

Многопоточность: В C++ нам доступны механизмы low-level, но лучше применить современный std::jthread, std::mutex, conditon_variable где нужно, либо использовать высокоуровневый folly futures или Intel TBB для Work Stealing thread pool. Однако, учитывая использование корутин, можно построить свой планировщик: один поток-реактор (Event Loop) плюс несколько воркеров. Корутины могут значительно упростить код, но нужно помнить о ограничения – лучше протестировать на невыпадающих утечках памяти, т.к. это относительно новая фича.

Безопасность и шифрование: Для TLS можно использовать библиотеку OpenSSL (или mbedTLS) напрямую, либо полагаться на gRPC (который поверх OpenSSL). Управление ключами (обмен публичными ключами узлов) можно делать вручную в настройках или автоматизировать (например, встроить простейший PKI – генерировать самоподписанные сертификаты).

Библиотеки для сериализации: Protocol Buffers, как упомянуто, или Cap'n Proto (ещё более быстрый, без десериализации). Также JSON (например, nlohmann/json lib для удобства). Для бинарных данных – возможно, стоит ограничиться передачей raw bytes (например, результаты могут передаваться как массив std::byte без преобразования, если транспорт поддерживает). Для определяемых пользователем типов – можно воспользоваться шаблонами сериализации (concept: Serializable) или заставлять пользователя регистрировать сериализаторы.

Управление зависимостями: Стоит использовать пакетный менеджер/сборщик (CMake + vcpkg или Conan) для подключения gRPC, Boost и т.п. Это облегчит воспроизводимость сборки.

Тестирование и отладка: Предусмотреть режим эмуляции кластера в одном процессе (для отладки), когда несколько узлов запускаются как потоки или даже корутины и общаются через inproc-провайдер. Это поможет отладить логику без сетевых задержек. Затем интеграционные тесты на реальных нескольких процессах с сетевым обменом.

Пример реализации CDN: Как частный случай применения (видео CDN) наш выбор технологий позволит:

Кодирование видео разбить на фреймы (DAG), распределить по узлам с нужными кодеками (метка узлов).

Потоковое выдача – через gRPC streaming или WebSocket.

Хранение сегментов – либо через нашу же систему (акторы-хранилища на узлах), либо подключив IPFS (можно spawn task, которая публикует файл в IPFS и возвращает hash).

Балансировка по гео – реализуется на уровне планировщика (напр., узлы имеют атрибут region, который сравнивается с region клиента).
Этот сценарий выполним при наличии всех перечисленных компонент.

Наконец, поэтапная реализация: рекомендуется сначала сделать минимально работающий прототип на одном узле (корутины, очередь задач, простой вызов), затем добавить поддержку нескольких узлов с тривиальным распределением (например, через прямые сокеты без полного консенсуса, просто по запросу), отладить передачу данных. После этого уже внедрять Raft-консенсус для надёжности и расширять функциональность (подзадачи, репликации). Такой поэтапный подход позволит тестировать и интегрировать лучшие решения из описанных технологий на каждой стадии, не усложняя сразу.

Заключение: Предложенная архитектура объединяет идеи из разных областей (кластерные менеджеры, volunteer computing, блокчейн, HPC) в единый универсальный фреймворк. Используя C++20 и современные библиотеки, мы сможем добиться как высокой производительности, так и богатой функциональности. В итоге получится система, способная эффективно распределять вычислительные задачи в децентрализованной среде, обеспечивая надёжность результатов и удобство для разработчиков приложений. Все ключевые требования – децентрализация, масштабируемость, скорость, отказоустойчивость, гибкость – будут достигнуты за счёт тщательно спроектированной технической концепции и правильного выбора технологий.
## Go implementation – Data Format & Serialization

- Фиксированный бинарный заголовок 64 байта для каждого кадра (`pkg/protocol/header.go`) — быстрый парсинг и маршрутизация на любом транспорте.
- Маркер формата полезной нагрузки: первый байт `Envelope.Payload` указывает сериализацию (`JSON`, `CBOR`, `Proto`). См. `pkg/protocol/body.go`.
- Кодеки: единый интерфейс и реализации в `pkg/protocol/codec`:
  - JSON через stdlib `encoding/json` (`application/json`)
  - CBOR через `fxamacker/cbor` (`application/cbor`)
  - Protobuf через `google.golang.org/protobuf` (`application/x-protobuf`)
- Фрейминг и фрагментация/сборка — `pkg/protocol/envelope.go`.
- Proto-схема для gRPC bi-stream и типизированных Task/Result — `pkg/protocol/proto/ttmesh.proto`. Генерация `make proto` (нужны `protoc`, `protoc-gen-go`, `protoc-gen-go-grpc`).

Поток данных: клиент сериализует Task выбранным кодеком → префиксует 1 байтом формата → кладёт в `Envelope.Payload` → отправляет. Получатель по первому байту выбирает кодек и десериализует в типизированное сообщение.

Тесты покрывают заголовок, фрейминг, фрагментацию и кодеки JSON/CBOR/Proto.
